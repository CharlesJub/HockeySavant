{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from selectolax.parser import HTMLParser\n",
    "from time import sleep\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(player_text):\n",
    "    \"\"\"\n",
    "    Clean and format a player's name extracted from the draft table.\n",
    "\n",
    "    Parameters:\n",
    "    player_text (str): The text representing a player's name, which may include roles like (D), (F), or (G).\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned and formatted player name without role indicators.\n",
    "\n",
    "    This function takes a player's name as input, which may include roles such as (D) for defense, (F) for forward, or (G) for goalie.\n",
    "    It removes these role indicators, trims any leading or trailing white spaces, and returns the cleaned and formatted player name.\n",
    "\n",
    "    Example:\n",
    "    original_name = \"John Smith (F)\"\n",
    "    cleaned_name = clean_name(original_name)\n",
    "    # cleaned_name will be \"John Smith\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the cleaned_name with the original text\n",
    "    cleaned_name = player_text.text().strip().split(' (')[0]\n",
    "    \n",
    "    # Iterate through roles_to_remove and remove them from the name\n",
    "    \n",
    "    \n",
    "    return cleaned_name\n",
    "\n",
    "def scrape_ep_draft(draft_year):\n",
    "    \"\"\"\n",
    "    Scrape data from Elite Prospects NHL Entry Draft page for a given draft year.\n",
    "\n",
    "    Parameters:\n",
    "    draft_year (int or str): The year of the NHL Entry Draft to scrape data for.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing draft data including pick number, team, player name, and links.\n",
    "\n",
    "    This function sends an HTTP GET request to the Elite Prospects website for the specified draft year.\n",
    "    It then extracts data from the draft table, including pick numbers, team names, player names, and links.\n",
    "    The data is organized into a DataFrame and returned for further analysis.\n",
    "\n",
    "    Example:\n",
    "    def clean_name(name):\n",
    "        # Implement your cleaning logic here\n",
    "        return cleaned_name\n",
    "    \"\"\"\n",
    "    # Construct the URL for Elite Prospects NHL Entry Draft page for the given 'draft_year'\n",
    "    draft_url = f\"https://www.eliteprospects.com/draft/nhl-entry-draft/{draft_year}\"\n",
    "    \n",
    "    # Send an HTTP GET request to the URL\n",
    "    resp = requests.get(draft_url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if resp.status_code == 200:\n",
    "        # Parse the HTML content of the response\n",
    "        html = HTMLParser(resp.text)\n",
    "        \n",
    "        # Extract the draft table element\n",
    "        draft_table = html.css_first('.players.table')\n",
    "        \n",
    "        # Extract data from different columns of the draft table\n",
    "        pick_number = [clean_name(overall_text) for overall_text in draft_table.css('td.overall')]\n",
    "        pick_team = [clean_name(team_text) for team_text in draft_table.css('td.team')]\n",
    "        pick_team_link = [team_node.css_first('a').attributes['href'] for team_node in draft_table.css('td.team')]\n",
    "        player_name = [clean_name(player_text) for player_text in draft_table.css('td.player')]\n",
    "        player_link = [player_node.css_first('a').attributes['href'] for player_node in draft_table.css('td.player')]\n",
    "        \n",
    "        # Create a DataFrame to store the extracted data\n",
    "        data = pd.DataFrame({'draft_year': draft_year,\n",
    "                             'pick_number': pick_number,\n",
    "                             'pick_team': pick_team,\n",
    "                             'pick_team_link': pick_team_link,\n",
    "                             'player_name': player_name,\n",
    "                             'player_link': player_link})\n",
    "        \n",
    "        # Return the DataFrame\n",
    "        return data\n",
    "    else:\n",
    "        # If the request was not successful, print the status code\n",
    "        print(f\"Got status code: {resp.status_code}\")\n",
    "\n",
    "def get_players_by_draft_year(draft_year):\n",
    "    \"\"\"\n",
    "    Scrape player data from Elite Prospects by draft year.\n",
    "\n",
    "    Args:\n",
    "        draft_year (int): The year of the NHL draft to retrieve player data for.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing player names and their corresponding links.\n",
    "    \"\"\"\n",
    "    draft_year_url = f\"https://www.eliteprospects.com/search/player?draft={draft_year}\"\n",
    "\n",
    "    player_names_full_draft_class = []\n",
    "    player_links_full_draft_class = []\n",
    "\n",
    "    # Make an HTTP request to the Elite Prospects website\n",
    "    resp = requests.get(draft_year_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if resp.status_code == 200:\n",
    "        # Parse the HTML response\n",
    "        html = HTMLParser(resp.text)\n",
    "        \n",
    "        # Extract information about the number of pages in the draft class\n",
    "        last_page_link = html.css_first('div.table-pagination').css('span')[1].css_first('a').attributes['href']\n",
    "        last_page = int(last_page_link[last_page_link.find('page=')+5:])\n",
    "        \n",
    "        # Extract player names and links from the first page\n",
    "        table = html.css_first(\".table.players\")\n",
    "        player_names = [clean_name(name) for name in table.css('td.name')]\n",
    "        player_links = [name.css_first('a').attributes['href'] for name in table.css('td.name')]\n",
    "        player_names_full_draft_class.extend(player_names)\n",
    "        player_links_full_draft_class.extend(player_links)\n",
    "\n",
    "        # Loop through the remaining pages using tqdm for a progress bar\n",
    "        for page_number in tqdm(range(2, last_page + 1), desc=f\"Scraping {draft_year} draft eligibles\"):\n",
    "            page_ending = f\"&page={page_number}\"\n",
    "            resp = requests.get(draft_year_url + page_ending)\n",
    "\n",
    "            # Handle 403 errors by waiting and retrying\n",
    "            while resp.status_code == 403:\n",
    "                print(f\"Waiting 100 seconds to resend request for page {page_number}\")\n",
    "                sleep(100)\n",
    "                resp = requests.get(draft_year_url + page_ending)\n",
    "\n",
    "            if resp.status_code == 200:\n",
    "                # Parse and extract data from the current page\n",
    "                html = HTMLParser(resp.text)\n",
    "                table = html.css_first(\".table.players\")\n",
    "                player_names = [clean_name(name) for name in table.css('td.name')]\n",
    "                player_links = [name.css_first('a').attributes['href'] for name in table.css('td.name')]\n",
    "                player_names_full_draft_class.extend(player_names)\n",
    "                player_links_full_draft_class.extend(player_links)\n",
    "            else:\n",
    "                print(f\"Request Failed. Status Code: {resp.status_code}. Page: {page_number}\")\n",
    "\n",
    "        # Create a DataFrame from the collected data\n",
    "        return pd.DataFrame({'player_name': player_names_full_draft_class,\n",
    "                             'player_link': player_links_full_draft_class})\n",
    "    else:\n",
    "        print(f\"Request Failed. Status Code: {resp.status_code}\")\n",
    "\n",
    "def scrape_skaters_ep_league(season, league):\n",
    "    \"\"\"\n",
    "    Scrape player data from Elite Prospects for a specific season and league.\n",
    "\n",
    "    Args:\n",
    "        season (str): The season for which you want to retrieve player data. Example: 2023-2024\n",
    "        league (str): The league for which you want to retrieve player data. Most popular leagues: 'NHL', 'AHL', 'ECHL', 'NCAA', 'WHL', 'OHL', 'QMJHL', 'USHL', 'KHL', 'SHL', 'LIIGA', 'NL', and 'CZECHIA'\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing player names and their corresponding links.\n",
    "    \"\"\"\n",
    "    league_url = f\"https://www.eliteprospects.com/league/{league}/stats/{season}\"\n",
    "\n",
    "    player_names_full_league = []\n",
    "    player_links_full_league = []\n",
    "    player_games_played_full_league = []\n",
    "    player_goals_full_league = []\n",
    "    player_assists_full_league = []\n",
    "    player_points_full_league = []\n",
    "    player_pim_full_league = []\n",
    "    player_plus_minus_full_league = []\n",
    "    # Make an HTTP request to the Elite Prospects website\n",
    "    resp = requests.get(league_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if resp.status_code == 200:\n",
    "        # Parse the HTML response\n",
    "        html = HTMLParser(resp.text)\n",
    "        \n",
    "        # Extract information about the number of pages in the league\n",
    "        last_page_link = html.css_first('div.table-pagination').css('span')[1].css_first('a').attributes['href']\n",
    "        last_page = int(last_page_link[last_page_link.find('page=')+5:])\n",
    "        \n",
    "        # Extract player names and links from the first page\n",
    "        table = html.css_first(\".table.player-stats\")\n",
    "        player_names = [clean_name(name) for name in table.css('td.player')]\n",
    "        player_links = [name.css_first('a').attributes['href'] if name.css_first('a') is not None else None for name in table.css('td.player')]\n",
    "        player_games_played = [clean_name(games_played) for games_played in table.css('td.gp')]\n",
    "        player_goals = [clean_name(goals) for goals in table.css('td.g')]\n",
    "        player_assists = [clean_name(assists) for assists in table.css('td.a')]\n",
    "        player_points = [clean_name(points) for points in table.css('td.tp')]\n",
    "        player_pim = [clean_name(pim) for pim in table.css('td.pim')]\n",
    "        plus_minus = [clean_name(plus_minus) for plus_minus in table.css('td.pm')]\n",
    "\n",
    "        player_names_full_league.extend(player_names)\n",
    "        player_links_full_league.extend(player_links)\n",
    "        player_games_played_full_league.extend(player_games_played)\n",
    "        player_goals_full_league.extend(player_goals)\n",
    "        player_assists_full_league.extend(player_assists)\n",
    "        player_points_full_league.extend(player_points)\n",
    "        player_pim_full_league.extend(player_pim)\n",
    "        player_plus_minus_full_league.extend(plus_minus)\n",
    "\n",
    "        # Loop through the remaining pages using tqdm for a progress bar\n",
    "        for page_number in tqdm(range(2, last_page + 1), desc=f\"Scraping {league}, {season} players\"):\n",
    "            page_ending = f\"&page={page_number}\"\n",
    "            resp = requests.get(league_url + page_ending)\n",
    "\n",
    "            # Handle 403 errors by waiting and retrying\n",
    "            while resp.status_code == 403:\n",
    "                print(f\"Waiting 100 seconds to resend request for page {page_number}\")\n",
    "                sleep(100)\n",
    "                resp = requests.get(league_url + page_ending)\n",
    "\n",
    "            if resp.status_code == 200:\n",
    "                # Parse and extract data from the current page\n",
    "                html = HTMLParser(resp.text)\n",
    "                table = html.css_first(\".table.player-stats\")\n",
    "                player_names = [clean_name(name) for name in table.css('td.player')]\n",
    "                player_links = [name.css_first('a').attributes['href'] if name.css_first('a') is not None else None for name in table.css('td.player')]\n",
    "                player_games_played = [clean_name(games_played) for games_played in table.css('td.gp')]\n",
    "                player_goals = [clean_name(goals) for goals in table.css('td.g')]\n",
    "                player_assists = [clean_name(assists) for assists in table.css('td.a')]\n",
    "                player_points = [clean_name(points) for points in table.css('td.tp')]\n",
    "                player_pim = [clean_name(pim) for pim in table.css('td.pim')]\n",
    "                plus_minus = [clean_name(plus_minus) for plus_minus in table.css('td.pm')]\n",
    "\n",
    "                player_names_full_league.extend(player_names)\n",
    "                player_links_full_league.extend(player_links)\n",
    "                player_games_played_full_league.extend(player_games_played)\n",
    "                player_goals_full_league.extend(player_goals)\n",
    "                player_assists_full_league.extend(player_assists)\n",
    "                player_points_full_league.extend(player_points)\n",
    "                player_pim_full_league.extend(player_pim)\n",
    "                player_plus_minus_full_league.extend(plus_minus)\n",
    "\n",
    "            else:\n",
    "                print(f\"Request Failed. Status Code: {resp.status_code}. Page: {page_number}\")\n",
    "        \n",
    "        # Create a DataFrame for all full league data\n",
    "        league_df = pd.DataFrame({'player_name': player_names_full_league,\n",
    "                                  'player_link': player_links_full_league,\n",
    "                                  'games_played':player_games_played_full_league,\n",
    "                                  'goals': player_goals_full_league,\n",
    "                                  'assists': player_assists_full_league,\n",
    "                                  'points': player_points_full_league,\n",
    "                                  'pim': player_pim_full_league,\n",
    "                                  'plus_minus': player_plus_minus_full_league})\n",
    "        \n",
    "        # Return a DataFrame without rows where 'player_link' is None\n",
    "        return league_df[league_df['player_link'] != None]\n",
    "    \n",
    "    else:\n",
    "        print(f\"Request Failed. Status Code: {resp.status_code}\")\n",
    "\n",
    "def scrape_nhl_draft(draft_year):\n",
    "    \"\"\"\n",
    "    Scrape NHL Draft records for a specific year.\n",
    "\n",
    "    Parameters:\n",
    "    - draft_year (int): The year of the NHL Draft records to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): A DataFrame containing the NHL Draft records for the specified year.\n",
    "    \"\"\"\n",
    "    # Create the URL for the NHL Draft records based on the input draft year.\n",
    "    draft_records_url = f\"https://records.nhl.com/site/api/draft?cayenneExp=draftYear={draft_year}\"\n",
    "    \n",
    "    # Send an HTTP GET request to the URL.\n",
    "    resp = requests.get(draft_records_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200).\n",
    "    if resp.status_code == 200:\n",
    "        # Parse the JSON response.\n",
    "        json_data = resp.json()\n",
    "        \n",
    "        # Convert the JSON data into a pandas DataFrame for easier manipulation.\n",
    "        df = pd.DataFrame(json_data['data'])\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # If the request was not successful, print an error message with the status code.\n",
    "        print(f\"Could not fetch data. Error code: {resp.status_code}\")\n",
    "\n",
    "def scrape_game_play_by_play(game_id):\n",
    "    \"\"\"\n",
    "    Scrape play-by-play data for a specific NHL game.\n",
    "\n",
    "    Parameters:\n",
    "    - game_id (int): The ID of the NHL game to retrieve play-by-play data for.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): A DataFrame containing the play-by-play data for the specified game.\n",
    "    \"\"\"\n",
    "    # Define the base URL for the NHL API.\n",
    "    base_url = \"https://api-web.nhle.com/v1/gamecenter\"\n",
    "\n",
    "    # Construct the play-by-play URL.\n",
    "    play_by_play_url = f\"{base_url}/{game_id}/play-by-play\"\n",
    "\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL.\n",
    "        resp = requests.get(play_by_play_url)\n",
    "        resp.raise_for_status()  # Raise an exception if there's an HTTP error.\n",
    "\n",
    "        # Parse the JSON response.\n",
    "        play_by_play_data = resp.json()\n",
    "\n",
    "        # Extract the 'plays' data and normalize it into a DataFrame.\n",
    "        df = pd.json_normalize(play_by_play_data['plays'])\n",
    "        # Clean \n",
    "        df = df.rename(columns=lambda x: x.replace('details.', ''))\n",
    "        df = df.rename(columns=lambda x: x.replace('periodDescriptor.', 'period_'))\n",
    "        df['homeTeam'] = play_by_play_data['homeTeam']['abbrev']\n",
    "        df['awayTeam'] = play_by_play_data['homeTeam']['abbrev']\n",
    "        return df\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any HTTP request errors.\n",
    "        print(f\"Failed to fetch play-by-play data: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_game_shift_report(game_id):\n",
    "    \"\"\"\n",
    "    Scrape the shift report for a specific NHL game.\n",
    "\n",
    "    Parameters:\n",
    "    - game_id (int): The ID of the NHL game to retrieve the shift report for.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): A DataFrame containing the shift report data for the specified game.\n",
    "    \"\"\"\n",
    "    # Define the base URL for the NHL API.\n",
    "    base_url = \"https://api.nhle.com/stats/rest/en/shiftcharts\"\n",
    "\n",
    "    # Construct the shift report URL.\n",
    "    shift_url = f\"{base_url}?cayenneExp=gameId={game_id}\"\n",
    "\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL.\n",
    "        resp = requests.get(shift_url)\n",
    "        resp.raise_for_status()  # Raise an exception if there's an HTTP error.\n",
    "\n",
    "        # Check if the response is successful (status code 200).\n",
    "        if resp.status_code == 200:\n",
    "            # Parse the JSON response.\n",
    "            shift_data = resp.json()\n",
    "\n",
    "            # Check if 'data' key is present in the response.\n",
    "            if 'data' in shift_data:\n",
    "                # Create a DataFrame from the 'data' key.\n",
    "                df = pd.DataFrame(shift_data['data'])\n",
    "                # Return shits that have a duration\n",
    "                return df[~(df['duration'].isna())]\n",
    "            else:\n",
    "                print(\"No 'data' key found in the response.\")\n",
    "        else:\n",
    "            print(\"Couldn't retrieve data. Status code:\", resp.status_code)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any HTTP request errors.\n",
    "        print(f\"Failed to fetch the shift report data: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_event_on_ice(df_pbp, df_shifts):\n",
    "    \"\"\"\n",
    "    Add 'homeOnIce' and 'awayOnIce' columns to df_pbp with players on ice for each event.\n",
    "\n",
    "    Parameters:\n",
    "    - df_pbp (pandas.DataFrame): DataFrame containing play-by-play data.\n",
    "    - df_shifts (pandas.DataFrame): DataFrame containing player shifts data.\n",
    "\n",
    "    Returns:\n",
    "    - df_pbp (pandas.DataFrame): Updated DataFrame with 'homeOnIce' and 'awayOnIce' columns.\n",
    "    \"\"\"\n",
    "    def get_on_ice(row, team):\n",
    "        \"\"\"\n",
    "        Helper function to get players on ice for a specific event and team.\n",
    "\n",
    "        Parameters:\n",
    "        - row (pandas.Series): The row representing an event.\n",
    "        - team (str): The team abbreviation (e.g., 'homeTeam' or 'awayTeam').\n",
    "\n",
    "        Returns:\n",
    "        - on_ice (array): Array of player IDs on ice for the event and team.\n",
    "        \"\"\"\n",
    "        on_ice = df_shifts[\n",
    "            (df_shifts['period'] == row['period']) &\n",
    "            (df_shifts['startTime'] < row['timeInPeriod']) &\n",
    "            (df_shifts['endTime'] >= row['timeInPeriod']) &\n",
    "            (df_shifts['teamAbbrev'] == team)\n",
    "        ]['playerId'].unique()\n",
    "        return on_ice\n",
    "\n",
    "    # Add 'homeOnIce' and 'awayOnIce' columns to df_pbp using the helper function.\n",
    "    df_pbp['homeOnIce'] = df_pbp.apply(lambda row: get_on_ice(row, row['homeTeam']), axis=1)\n",
    "    df_pbp['awayOnIce'] = df_pbp.apply(lambda row: get_on_ice(row, row['awayTeam']), axis=1)\n",
    "\n",
    "    return df_pbp\n",
    "\n",
    "def find_game_ids(data):\n",
    "    \"\"\"\n",
    "    Recursively find all instances of \"gamePk\" in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): The dictionary to search for \"gamePk\".\n",
    "\n",
    "    Returns:\n",
    "    - game_pks (list): A list of all \"gamePk\" values found in the dictionary.\n",
    "    \"\"\"\n",
    "    game_pks = []\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if key == \"gamePk\":\n",
    "                game_pks.append(value)\n",
    "            elif isinstance(value, (dict, list)):\n",
    "                game_pks.extend(find_game_ids(value))\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            game_pks.extend(find_game_ids(item))\n",
    "\n",
    "    return game_pks\n",
    "\n",
    "def scrape_season_game_ids(season, game_type=\"R\"):\n",
    "    \"\"\"\n",
    "    Scrape NHL game IDs for a specific season and game type.\n",
    "\n",
    "    Parameters:\n",
    "    - season (str): The season in the format 'YYYYYYYY' (e.g., '20232024').\n",
    "    - game_type (str, optional): The game type ('R' for regular season by default). 'PR' for pre-season and 'P' for playoffs.\n",
    "    All game types can be found: https://statsapi.web.nhl.com/api/v1/gameTypes.\n",
    "\n",
    "    Returns:\n",
    "    - game_ids (list): A list of game IDs for the specified season and game type.\n",
    "    \"\"\"\n",
    "    # Construct the URL for the NHL schedule API.\n",
    "    season_url = f\"https://statsapi.web.nhl.com/api/v1/schedule?season={season}&gameType={game_type}\"\n",
    "\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL.\n",
    "        resp = requests.get(season_url)\n",
    "        resp.raise_for_status()  # Raise an exception if there's an HTTP error.\n",
    "\n",
    "        if resp.status_code == 200:\n",
    "            # Parse the JSON response.\n",
    "            data = resp.json()\n",
    "\n",
    "            # Find game IDs using a recursive function (assuming you have the 'find_game_pks' function).\n",
    "            game_ids = find_game_ids(data)\n",
    "\n",
    "            return game_ids\n",
    "        else:\n",
    "            print(f\"Failed to fetch data. Status code: {resp.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any HTTP request errors.\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "\n",
    "    return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:12<00:00,  1.24it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "concat() got an unexpected keyword argument 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\charl\\OneDrive\\Desktop\\Projects\\HockeySavant\\draft.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/charl/OneDrive/Desktop/Projects/HockeySavant/draft.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/charl/OneDrive/Desktop/Projects/HockeySavant/draft.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m season \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m20222023\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/charl/OneDrive/Desktop/Projects/HockeySavant/draft.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m combined_dataframe \u001b[39m=\u001b[39m scrape_plays_season(season)\n",
      "\u001b[1;32mc:\\Users\\charl\\OneDrive\\Desktop\\Projects\\HockeySavant\\draft.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/charl/OneDrive/Desktop/Projects/HockeySavant/draft.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     combined_dfs\u001b[39m.\u001b[39mappend(df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/charl/OneDrive/Desktop/Projects/HockeySavant/draft.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Concatenate the list of DataFrames into one large DataFrame\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/charl/OneDrive/Desktop/Projects/HockeySavant/draft.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(combined_dfs, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, columns\u001b[39m=\u001b[39;49mcolumn_names)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/charl/OneDrive/Desktop/Projects/HockeySavant/draft.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "\u001b[1;31mTypeError\u001b[0m: concat() got an unexpected keyword argument 'columns'"
     ]
    }
   ],
   "source": [
    "def scrape_plays_season(season):\n",
    "    game_ids = scrape_season_game_ids(season)\n",
    "    game_ids = game_ids[:16]\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    combined_dfs = []\n",
    "    column_names = ['hitteePlayerId', 'typeCode', 'blockingPlayerId', 'period', 'winningPlayerId', 'scoringPlayerId', 'awayScore',\n",
    "                    'awaySOG', 'servedByPlayerId', 'awayOnIce', 'period_number', 'assist2PlayerId', 'homeOnIce', 'committedByPlayerId',\n",
    "                    'secondaryReason', 'timeInPeriod', 'homeTeamDefendingSide', 'timeRemaining', 'homeScore', 'awayTeam', 'xCoord',\n",
    "                    'reason', 'period_periodType', 'sortOrder', 'assist1PlayerId', 'situationCode', 'homeTeam', 'typeDescKey',\n",
    "                    'drawnByPlayerId', 'zoneCode', 'duration', 'descKey', 'shootingPlayerId', 'eventId', 'losingPlayerId',\n",
    "                    'eventOwnerTeamId', 'yCoord', 'homeSOG', 'shotType', 'goalieInNetId', 'hittingPlayerId', 'playerId']\n",
    "    for game in tqdm(game_ids):\n",
    "        pbp_df = scrape_game_play_by_play(game)\n",
    "        shifts_df = scrape_game_shift_report(game)\n",
    "        df = get_event_on_ice(pbp_df, shifts_df)\n",
    "                \n",
    "        # Append the DataFrame to the list\n",
    "        combined_dfs.append(df)\n",
    "\n",
    "    # Concatenate the list of DataFrames into one large DataFrame\n",
    "    df = pd.concat(combined_dfs, axis=1, columns=column_names)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "season = '20222023'\n",
    "combined_dataframe = scrape_plays_season(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received: {\"type\":\"html\",\"target\":\"#skatingdistance-section-content\",\"html\":\"<div class=\\\"col-lg-6 col-md-6\\\">\\n    <div class=\\\"table-responsive\\\">\\n\\n        <table class=\\\"table table-hover\\\">\\n            <thead>\\n                <tr>\\n                    <td scope=\\\"col\\\"></td>\\n                    <td scope=\\\"col\\\"></td>\\n                    <td scope=\\\"col\\\" class=\\\"text-center\\\">League average<br>by position (F/D)</td>\\n                    <td scope=\\\"col\\\" class=\\\"text-center\\\">Percentile</td>\\n                </tr>\\n            </thead>\\n            <tbody>\\n        \\n                        <tr>\\n                            <td scope=\\\"row\\\">Total (mi)</td>\\n                                <th class=\\\"text-center\\\">6.36</th>\\n                            <th class=\\\"text-center\\\">11.11</th>\\n                            <th class=\\\"text-center\\\">Below 50th</td>\\n                        </tr>\\n                        <tr>\\n                            <td scope=\\\"row\\\">Average Per 60 (mi)</td>\\n                                <th class=\\\"text-center\\\">9.42</th>\\n                            <th class=\\\"text-center\\\">9.70</th>\\n                            <th class=\\\"text-center\\\">Below 50th</td>\\n                        </tr>\\n                        <tr>\\n                            <td scope=\\\"row\\\">Top Game (mi)</td>\\n                                <th class=\\\"text-center\\\"><span data-html=\\\"true\\\" data-tooltip=\\\"10/21/2023 @ ARI\\\">3.26</span></th>\\n                            <th class=\\\"text-center\\\">2.71</th>\\n                            <th class=\\\"text-center\\\">76</td>\\n                        </tr>\\n                        <tr>\\n                            <td scope=\\\"row\\\">Top Period (mi)</td>\\n                                <th class=\\\"text-center\\\"><span data-html=\\\"true\\\" data-tooltip=\\\"10/19/2023 vs DAL - Period 2\\\">1.22</span></th>\\n                            <th class=\\\"text-center\\\">1.05</th>\\n                            <th class=\\\"text-center\\\">71</td>\\n                        </tr>\\n        \\n            </tbody>\\n        </table>\\n    </div>\\n</div>\\n<div class=\\\"col-lg-6 col-md-6\\\">\\n    <div class=\\\"component-container\\\">\\n\\n            <sl-webc-date-bar-chart id=\\\"skatingdistance-datebarchart\\\" data-json=\\\"{&quot;chartData&quot;:[{&quot;date&quot;:&quot;2023-10-21&quot;,&quot;value&quot;:3.26,&quot;tooltip&quot;:[&quot;Distance skated: 3.26 miles&quot;,&quot;10/21/2023 @ ARI&quot;,&quot;TOI: 21:33&quot;]},{&quot;date&quot;:&quot;2023-10-19&quot;,&quot;value&quot;:3.1,&quot;tooltip&quot;:[&quot;Distance skated: 3.10 miles&quot;,&quot;10/19/2023 vs DAL&quot;,&quot;TOI: 19:00&quot;]}],&quot;config&quot;:{&quot;maxValue&quot;:0,&quot;minDate&quot;:&quot;2023-10-01&quot;,&quot;maxDate&quot;:&quot;2023-10-22&quot;,&quot;monthLabels&quot;:[&quot;Jan&quot;,&quot;Feb&quot;,&quot;Mar&quot;,&quot;Apr&quot;,&quot;May&quot;,&quot;Jun&quot;,&quot;Jul&quot;,&quot;Aug&quot;,&quot;Sep&quot;,&quot;Oct&quot;,&quot;Nov&quot;,&quot;Dec&quot;]}}\\\"></sl-webc-shot-chart>\\n\\n    </div>\\n</div>\",\"callback\":\"runClientFns\"}\n"
     ]
    }
   ],
   "source": [
    "# Start work on scraping edge data\n",
    "import asyncio\n",
    "from websockets.sync.client import connect\n",
    "import json\n",
    "\n",
    "def hello():\n",
    "    with connect(\"wss://edge.nhl.com/en/skater/8484153\") as websocket:\n",
    "        payload = {\"type\":\"action\",\"event\":{\"domain\":\"edge.nhl.com\",\"uri\":\"/en/skater/8484153\",\"action\":\"load\",\"data\":{\"renderFunction\":\"renderProfileContent\",\"target\":\"#skatingdistance-section-content\",\"params\":{\"sectionName\":\"skatingdistance\",\"units\":\"imperial\",\"manpower\":\"all\",\"season\":\"20232024\",\"stage\":\"regular\",\"feed\":\"skatersProfiles\",\"id\":\"8484153\"},\"callbackFunction\":\"runClientFns\"}}}\n",
    "        websocket.send(json.dumps(payload))\n",
    "        message = websocket.recv()\n",
    "        print(f\"Received: {message}\")\n",
    "\n",
    "hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
